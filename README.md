## Description

In this project we build a naive bayesian text-based classifier that predicts the category of a student project based on its text description. The project is extended by transforming the data by document frequency. This extension to the base model requires hyper-parameter tuning. A more detailed description of each part of the project follows.

### Preprocessing & Standard Naive Bayesian Learner
We first sort each example into a bag of words. Each "bag" is a list and is stored in a dictionary with the index of the example as the key. We allow for duplicate words. During this process we also keep track of: the number of occurances of each word in a class; the total number of words per class; the quantities of each class. This should give us all the ingredients we need for our standard naive Bayesian (SNB) classifier.

Earlier versions used an attribute matrix instead of the bags of words method, but it was harder to work with while building the classifier and offered no noticable computational benefits. Bags-of-words is easier to implement, easier to work with (as one can simply iterate through the examples and the words within the examples), and not computationally expensive. Generating these bags at this stage will also make future improvements (e.g. adding n-grams) easier to implement.

The various frequency counts are the most important part of this preprocessing section for the training phase. Total word counts and class counts are stored in lists of length 4, with indices corresponding to classes (0 for "A", 1 for "S", etc). Numbers of occurances of each word per class are stored in a dictionary where each word maps to a list of length 4 (indexed as previously described). This dictionary also serves as the vocabulary.

All the above is done in our track_frequencies() function, we now enter the train() function which calculates all the probabilities from the results of the previous function. We first calculate our prior probabilities (class count divided by number of examples) and store these in a dictionary. We then calculate our conditional probabilities (with laplace smoothing where m=|X| is the number of unique words so that words that don't appear in a class don't render that value useless). These are stored in a dictionary with each word mapped to a list of its conditional probability with each class.

Examples can now be predicted (with predict_all() and predict_example() functions) using the SNB formula (with laplace smoothing for unseen words). Note that each time we multiply by a conditional probability, we also multiply by a constant in order to keep our result in a reasonable range for our computer to store (experimentation found c=500 to work). For a given example we multiply each likelyhood by the same amount, so this operation doesn't change the final prediction.


### Evaluation: Validation

We implement a standard k-cross validation method. We split the data into k folds and allow for the final fold to be smaller than the rest in the case that k doesn't divide our number of examples evenly. The training folds are then concatenated and trained (on either our standard model or improved model), validated against the validation fold (using the Accuracy metric), and stored in a list. This is repeated for each fold and the final accuracy score is returned as the mean of each of the validation folds' accuracy. This method gives us a fairly accurate estimate of the test error we can expect (especially with high k), but is somewhat computationally expensive.


### Model Improvement: Transforming by Document Frequency

Section 4.2 of the paper "Tackling the Poor Assumptions of Naive Bayes Text Classifiers" (J. Rennie, L. Shih, J. Teevan, and D. Karger, available at https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) describes how word frequency in a document, $f_i$, can be scaled as $f_i^\prime = f_i \log \frac{\sum_j 1}{\sum_j \delta_{ij}}$ where $\delta_{ij}$ is 1 if word $i$ occurs in document $j$, 0 otherwise. This downweights words that appear in many documents and upweight words that appear in few. The base of the logarithm is a hyper-parameter, a higher base will penalise commonly appearing words more. Intuitively, this seems like it would improve performance. Words such as "we", "that", and "to" appear often in training data and don't seem to correlate to any particular class, but if such words happen to occur more in a particular class in our training data this would create bias and hurt performance. Less common words are more likely to be domain-specific and be effective at predicting that class.

Implementing this improvement requires us to change the way we track our various frequencies. We want to first track the number of documents a given word occurs in. Then, instead of recording each frequency as normal, we instead first perform the transformation and take this as our word frequency. We then return everything in the same format as our standard preprocessing from before. This allows us to use the same prediction functions, but now they are predicting based on our transformed data.

We can now explore how to optimise our logarithm base, b, hyper-parameter. Some quick expermentation found that b=e gives $\approx 96\%$ validation score, b=100,000 gives $\approx 98\%$, and b=1,000,000 gives $\approx 97\%$. If we make the assumption that larger bases will increase our accuracy to a maximum, and then accuracy scores will decrease, we can make an algorithm that can find this maximum value. We can modify a basic binary search algorithm (with e.g. e as a minimum and 1,000,000 as a maximum) to check if accuracy is increasing or decreasing at a mid point (by validating at this point and at some small $\varepsilon$ distance nearby) and change our search range accordingly. We can run this in a linear space or logspace (which may be better due to the scale of the numbers we are dealing with). As validation scores are expensive, we will only do this a small number of times. Testing a small number of hyper-parameter values will also minimise our risk of optimisation bias. Running this in logspace using 10-fold cross validation and using 10 "guesses" we get an optimised b of 158405 with validation score $\approx 98.11\%$, which is near to the 100,000 guess from earlier.

### Validation Results

We use 10-fold cross validation so that computations complete in reasonable time. Our SNB model achieves a validation score of $\approx 96.0\%$ and our improved model with optimised hyper-parameter achieves a validation score of $\approx 98.1\%$ which is a notable improvement as expected. As previously mentioned, our hyper-parameter optimisation relies on the assumption that we have a single global maximum and this may not be true. It is possible algorithm that is more robust to local maxima may find a better base for our logarithm, but this method with our assumption still manages to improve performance.