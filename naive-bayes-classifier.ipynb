{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of:\n",
    "# 1) number of occurances of each word in a class (\"all_words\" dictionary)\n",
    "# 2) the total number of words per class (\"total_frequency_by_class\" list)\n",
    "# 3) quantities of each class (\"numbers_of_classes\" list)\n",
    "\n",
    "\n",
    "def empty_class_list():\n",
    "    return [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "def track_frequencies(data, num_examples, class_indexes, b=None):\n",
    "\n",
    "    numbers_of_classes = [0, 0, 0, 0]\n",
    "    total_frequency_by_class = [0, 0, 0, 0]\n",
    "    all_words = defaultdict(empty_class_list)\n",
    "    examples_bags = {}\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        word_list = data.iloc[i][\"Description\"].split()  # note that there is no punctuation in the training data, we also consider duplicates.\n",
    "        examples_bags[i] = word_list\n",
    "\n",
    "        class_index = class_indexes[data.iloc[i][\"Class\"]]\n",
    "\n",
    "        numbers_of_classes[class_index] += 1\n",
    "\n",
    "        for word in word_list:\n",
    "            all_words[word][class_index] += 1\n",
    "            total_frequency_by_class[class_index] += 1\n",
    "\n",
    "    return numbers_of_classes, total_frequency_by_class, all_words, examples_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_frequencies_and_transform(data, num_examples, class_indexes, b):  # Uses transforming by document frequency\n",
    "\n",
    "    numbers_of_classes = [0, 0, 0, 0]\n",
    "    total_frequency_by_class = [0, 0, 0, 0]\n",
    "    all_words = defaultdict(empty_class_list)\n",
    "    words_document_occurances = Counter()\n",
    "    examples_bags = {}\n",
    "\n",
    "    for i in range(num_examples):\n",
    "\n",
    "        word_list = data.iloc[i][\"Description\"].split()  # note that there is no punctuation in the training data, we also consider duplicates.\n",
    "        class_index = class_indexes[data.iloc[i][\"Class\"]]\n",
    "        examples_bags[i] = word_list\n",
    "\n",
    "        word_set = list(set(word_list))\n",
    "        words_document_occurances.update(word_set)  # this needs to be generated before we can calculate our frequency transformations\n",
    "\n",
    "        numbers_of_classes[class_index] += 1\n",
    "\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        example_bag = examples_bags[i]\n",
    "\n",
    "        class_index = class_indexes[data.loc[i, \"Class\"]]\n",
    "\n",
    "        words_freq_in_document = Counter(example_bag)\n",
    "\n",
    "        for word in words_freq_in_document:\n",
    "            transformed_freq = words_freq_in_document[word] * math.log(num_examples / words_document_occurances[word], b)\n",
    "\n",
    "            all_words[word][class_index] += transformed_freq\n",
    "            total_frequency_by_class[class_index] += transformed_freq\n",
    "\n",
    "\n",
    "    return numbers_of_classes, total_frequency_by_class, all_words, examples_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_examples, project_classes, class_indexes, numbers_of_classes, total_frequency_by_class, all_words):  # calculates all probabilities\n",
    "\n",
    "    # Prior probabilities\n",
    "    priors = {project_classes[i]: numbers_of_classes[i] / num_examples for i in range(len(project_classes))}\n",
    "\n",
    "    # Conditional probabilities\n",
    "    laplace_top = 1\n",
    "    laplace_bottom = len(all_words)\n",
    "\n",
    "    conditionals = defaultdict(empty_class_list)\n",
    "    for word in all_words:\n",
    "\n",
    "        word_counts = all_words[word]\n",
    "\n",
    "        for class_index in class_indexes.values():\n",
    "\n",
    "            word_count = word_counts[class_index]\n",
    "            class_length = total_frequency_by_class[class_index]\n",
    "\n",
    "            conditionals[word][class_index] += (word_count + laplace_top) / (class_length + laplace_bottom)\n",
    "\n",
    "    return priors, conditionals, laplace_top, laplace_bottom \n",
    "    # we pass the laplace values between functions so we don't need to keep recalculating them (and easier if we want to experiment with them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example(example_bag, priors, conditionals, class_indexes, laplace_top, laplace_bottom):  # Using the naive bayesian algorithm\n",
    "\n",
    "    likelyhoods = np.zeros(4)\n",
    "\n",
    "    for project_class in class_indexes:\n",
    "\n",
    "        prior = priors[project_class]\n",
    "\n",
    "        conditionals_product = 1\n",
    "        for word in example_bag:\n",
    "            if word in conditionals:\n",
    "                conditionals_product *= conditionals[word][class_indexes[project_class]]\n",
    "                conditionals_product *= 500  # multiply by constant so the computer can actually store the final answer\n",
    "            else:\n",
    "                conditionals_product *= (laplace_top / laplace_bottom)  # laplace smoothing added for previously unseen words\n",
    "            \n",
    "        likelyhoods[class_indexes[project_class]] = prior * conditionals_product\n",
    "        #print(prior * conditionals_product)\n",
    "        \n",
    "\n",
    "    prediction_index = np.argmax(likelyhoods)\n",
    "\n",
    "    return list(class_indexes.keys())[prediction_index]\n",
    "    \n",
    "\n",
    "def predict_all(examples_bags, priors, conditionals, class_indexes, laplace_top, laplace_bottom):  # predicts all examples given dictionary of bags\n",
    "    Y_list = []\n",
    "\n",
    "    for example_bag in examples_bags.values():\n",
    "        Y_list.append(predict_example(example_bag, priors, conditionals, class_indexes, laplace_top, laplace_bottom))\n",
    "\n",
    "    return pd.DataFrame({\"Class\": Y_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bags(data):\n",
    "    num_examples = len(data)\n",
    "\n",
    "    examples_bags = {}\n",
    "    for i in range(num_examples):\n",
    "        word_list = data.iloc[i][\"Description\"].split()  # note that there is no punctuation in the training data, we also consider duplicates.\n",
    "        examples_bags[i] = word_list\n",
    "\n",
    "    return examples_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_folds(data, k):\n",
    "    data_size = len(data)\n",
    "    fold_size = math.ceil(data_size / k)\n",
    "    folds = {}\n",
    "\n",
    "    for i in range(k):\n",
    "        kth_fold = data.iloc[i*fold_size:(i+1)*fold_size]\n",
    "        folds[i+1] = kth_fold\n",
    "\n",
    "    return folds, fold_size\n",
    "\n",
    "\n",
    "def accuracy(Y_true, Y_pred):\n",
    "    total = len(Y_true)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    for i in range(total):\n",
    "        if Y_true[i] == Y_pred[i]:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    return correct_predictions / total\n",
    "\n",
    "\n",
    "def kcross_validation(data, k, project_classes, class_indexes, b, with_transform=True):\n",
    "    folds, fold_size = split_into_folds(data, k)\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        training_folds = [folds[fold] for fold in folds if i+1 != fold]\n",
    "        data_train = pd.concat(training_folds)\n",
    "        training_size = len(data_train)\n",
    "        data_validation = folds[i+1]\n",
    "        \n",
    "        if not with_transform:\n",
    "            numbers_of_classes, total_frequency_by_class, all_words, examples_bags = track_frequencies(data_train, training_size, class_indexes)\n",
    "        else:\n",
    "            numbers_of_classes, total_frequency_by_class, all_words, examples_bags = track_frequencies_and_transform(data, training_size, class_indexes, b)\n",
    "\n",
    "        priors, conditionals, laplace_top, laplace_bottom = train(training_size, project_classes, class_indexes, numbers_of_classes, total_frequency_by_class, all_words)\n",
    "        validation_bags = prepare_bags(data_validation)\n",
    "\n",
    "        validation_prediction = predict_all(validation_bags, priors, conditionals, class_indexes, laplace_top, laplace_bottom)\n",
    "\n",
    "        acc = accuracy(list(data_validation[\"Class\"]), list(validation_prediction[\"Class\"]))\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    return statistics.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_base(lower, upper, max_tries, epsilon_scalar, data_train, k, project_classes, class_indexes, in_logspace=True):  # modified binary search\n",
    "\n",
    "    tries = 0\n",
    "    while tries < max_tries:\n",
    "\n",
    "        if in_logspace:  # if we have a massive range e.g. e~2.7 to 1,000,000 then this may be more effective\n",
    "            lower_base = math.log(lower, 10)\n",
    "            upper_base = math.log(upper, 10)\n",
    "            mid = np.logspace(lower_base, upper_base, num = 3)[1]\n",
    "            #print(str(np.logspace(lower_base, upper_base, num = 3)))\n",
    "        else:\n",
    "            lower = lower\n",
    "            upper = upper\n",
    "            mid = (lower + upper) / 2\n",
    "            #print(str(lower) + \" \" + str(mid) + \" \" + str(upper))\n",
    "\n",
    "        epsilon = epsilon_scalar * (upper - mid)  # scaling our epsilon      \n",
    "\n",
    "        mid_acc = kcross_validation(data_train, k, project_classes, class_indexes, mid)\n",
    "        mid_plus_e_acc = kcross_validation(data_train, k, project_classes, class_indexes, mid+epsilon)\n",
    "        tries += 2\n",
    "\n",
    "        if mid_acc < mid_plus_e_acc:  # accuracy is increasing after this point\n",
    "            lower = mid + epsilon\n",
    "        else:  # accuracy is decreasing after this point \n",
    "            upper = mid\n",
    "\n",
    "    return mid, kcross_validation(data_train, k, project_classes, class_indexes, mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_data(data_new, data_train, training_size, project_classes, class_indexes, b, with_transform=True):\n",
    "    \n",
    "    if not with_transform:\n",
    "        numbers_of_classes, total_frequency_by_class, all_words, examples_bags = track_frequencies(data_train, training_size, class_indexes)\n",
    "    else:\n",
    "        numbers_of_classes, total_frequency_by_class, all_words, examples_bags = track_frequencies_and_transform(data_train, training_size, class_indexes, b)\n",
    "    \n",
    "    priors, conditionals, laplace_top, laplace_bottom = train(training_size, project_classes, class_indexes, numbers_of_classes, total_frequency_by_class, all_words)\n",
    "    data_new_bags = prepare_bags(data_new)\n",
    "\n",
    "    final_prediction = predict_all(data_new_bags, priors, conditionals, class_indexes, laplace_top, laplace_bottom)\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "file_path_train = \"./data/train.csv\"\n",
    "data_train = pd.read_csv(file_path_train)\n",
    "project_classes = [\"A\", \"S\", \"G\", \"W\"]\n",
    "class_indexes = {proj_class: index for index, proj_class in enumerate(project_classes)}\n",
    "training_size = len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(158405.4522032182), 0.9811363636363636)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to optimise the base hyper-parameter for the method of Transforming by Document Frequency\n",
    "\n",
    "k = 10  # number of folds in k-fold validation\n",
    "best_base = optimise_base(math.e, 1000000, 10, 0.1, data_train, k, project_classes, class_indexes, True)\n",
    "print(best_base)  # prints the best base found and its validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9604545454545454\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to find validation accuracy for either basic naive bayes or naive bayes with transformation\n",
    "\n",
    "with_tranformation = False  # basic naive bayes or bayes with transformation\n",
    "k = 10  # number of folds in k-fold validation\n",
    "b = math.e  # base hyper-parameter for the method of Transforming by Document Frequency\n",
    "validation_score = kcross_validation(data_train, k, project_classes, class_indexes, b, with_tranformation)\n",
    "print(validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to predict a set of new examples\n",
    "\n",
    "file_path_test = \"./data/test.csv\"\n",
    "data_test = pd.read_csv(file_path_test)\n",
    "\n",
    "b = 158405  # found using our hyper-parameter optimisation\n",
    "\n",
    "use_extended_version_with_transform = True\n",
    "final_prediction = predict_new_data(data_test, data_train, training_size, project_classes, class_indexes, b, use_extended_version_with_transform)\n",
    "\n",
    "final_prediction.index = range(1, len(final_prediction) + 1)\n",
    "final_prediction.index.name = 'Id'\n",
    "final_prediction.to_csv('final_prediction.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Standard Naive Bayesian Learner\n",
    "We first sort each example into a bag of words. Each \"bag\" is a list and is stored in a dictionary with the index of the example as the key. We allow for duplicate words, this is consistent with the examples shown in lectures. During this process we also keep track of: the number of occurances of each word in a class; the total number of words per class; the quantities of each class. This should give us all the ingredients we need for our standard naive Bayesian (SNB) classifier.\n",
    "\n",
    "Earlier versions used an attribute matrix instead of the bags of words method, but it was harder to work with while building the classifier and offered no noticable computational benefits. Bags-of-words is easier to implement, easier to work with (as one can simply iterate through the examples and the words within the examples), and not computationally expensive. Generating these bags at this stage will also make future improvements (e.g. adding n-grams) easier to implement.\n",
    "\n",
    "The various frequency counts are the most important part of this preprocessing section for the training phase. Total word counts and class counts are stored in lists of length 4, with indices corresponding to classes (0 for \"A\", 1 for \"S\", etc). Numbers of occurances of each word per class are stored in a dictionary where each word maps to a list of length 4 (indexed as previously described). This dictionary also serves as the vocabulary.\n",
    "\n",
    "All the above is done in our track_frequencies() function, we now enter the train() function which calculates all the probabilities from the results of the previous function. We first calculate our prior probabilities (class count divided by number of examples) and store these in a dictionary. We then calculate our conditional probabilities (with laplace smoothing where m=|X| is the number of unique words so that words that don't appear in a class don't render that value useless). These are stored in a dictionary with each word mapped to a list of its conditional probability with each class.\n",
    "\n",
    "Examples can now be predicted (with predict_all() and predict_example() functions) using the SNB formula (with laplace smoothing for unseen words). Note that each time we multiply by a conditional probability, we also multiply by a constant in order to keep our result in a reasonable range for our computer to store (experimentation found c=500 to work). For a given example we multiply each likelyhood by the same amount, so this operation doesn't change the final prediction.\n",
    "\n",
    "\n",
    "### Evaluation: Validation\n",
    "\n",
    "We implement a standard k-cross validation method. We split the data into k folds and allow for the final fold to be smaller than the rest in the case that k doesn't divide our number of examples evenly. The training folds are then concatenated and trained (on either our standard model or improved model), validated against the validation fold (using the Accuracy metric), and stored in a list. This is repeated for each fold and the final accuracy score is returned as the mean of each of the validation folds' accuracy. This method gives us a fairly accurate estimate of the test error we can expect (especially with high k), but is somewhat computationally expensive.\n",
    "\n",
    "\n",
    "### Model Improvement: Transforming by Document Frequency\n",
    "\n",
    "Section 4.2 of the paper \"Tackling the Poor Assumptions of Naive Bayes Text Classifiers\" (J. Rennie, L. Shih, J. Teevan, and D. Karger, available at https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) describes how word frequency in a document, $f_i$, can be scaled as $f_i^\\prime = f_i \\log \\frac{\\sum_j 1}{\\sum_j \\delta_{ij}}$ where $\\delta_{ij}$ is 1 if word $i$ occurs in document $j$, 0 otherwise. This downweights words that appear in many documents and upweight words that appear in few. The base of the logarithm is a hyper-parameter, a higher base will penalise commonly appearing words more. Intuitively, this seems like it would improve performance. Words such as \"we\", \"that\", and \"to\" appear often in training data and don't seem to correlate to any particular class, but if such words happen to occur more in a particular class in our training data this would create bias and hurt performance. Less common words are more likely to be domain-specific and be effective at predicting that class.\n",
    "\n",
    "Implementing this improvement requires us to change the way we track our various frequencies. We want to first track the number of documents a given word occurs in. Then, instead of recording each frequency as normal, we instead first perform the transformation and take this as our word frequency. We then return everything in the same format as our standard preprocessing from before. This allows us to use the same prediction functions, but now they are predicting based on our transformed data.\n",
    "\n",
    "We can now explore how to optimise our logarithm base, b, hyper-parameter. Some quick expermentation found that b=e gives $\\approx 96\\%$ validation score, b=100,000 gives $\\approx 98\\%$, and b=1,000,000 gives $\\approx 97\\%$. If we make the assumption that larger bases will increase our accuracy to a maximum, and then accuracy scores will decrease, we can make an algorithm that can find this maximum value. We can modify a basic binary search algorithm (with e.g. e as a minimum and 1,000,000 as a maximum) to check if accuracy is increasing or decreasing at a mid point (by validating at this point and at some small $\\varepsilon$ distance nearby) and change our search range accordingly. We can run this in a linear space or logspace (which may be better due to the scale of the numbers we are dealing with). As validation scores are expensive, we will only do this a small number of times. Testing a small number of hyper-parameter values will also minimise our risk of optimisation bias. Running this in logspace using 10-fold cross validation and using 10 \"guesses\" we get an optimised b of 158405 with validation score $\\approx 98.11\\%$, which is near to the 100,000 guess from earlier.\n",
    "\n",
    "### Validation Results\n",
    "\n",
    "We use 10-fold cross validation so that computations complete in reasonable time. Our SNB model achieves a validation score of $\\approx 96.0\\%$ and our improved model with optimised hyper-parameter achieves a validation score of $\\approx 98.1\\%$ which is a notable improvement as expected. As previously mentioned, our hyper-parameter optimisation relies on the assumption that we have a single global maximum and this may not be true. It is possible algorithm that is more robust to local maxima may find a better base for our logarithm, but this method with our assumption still manages to improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "361ass1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
